{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Like a bowling ball on a skating rink, the black geodesic sphere of the East Greenland Ice-Core Project's communal living space stands out against the endless white nothingness of the Greenland ice sheet.\\n\\nBut the real action at East GRIP is under the surface. Researchers are drilling through more than 2.5 kilometers of ice, down to the bedrock below. The ice is sliding fast - for a glacier - toward the sea. Scientists here want to know why. The answer may hold clues to the future of the world's coastal cities.\\n\\nGreenland is melting. As it melts, it adds roughly 1 millimeter of water per year to global sea levels. And the pace of melting is quickening.\\n\\nIf all the ice covering the world's largest island were to thaw, sea levels would rise roughly 6 meters. Scientists don't know how fast, or how likely, that is to happen. East GRIP is looking for evidence to inform both those questions.\\n\\nThe answers are a matter of growing urgency. The seas are rising faster. And the same processes at work on Greenland's glaciers at the top of the world could send vast sections of Antarctica's ice sheet into the sea as well, raising ocean levels even further.\\n\\nThe Arctic is warming twice as fast as the rest of the planet. Scientists studying the rapid changes gather in the small Greenland town of Kangerlussuaq, a former U.S. military base built during World War II. Through the Cold War, this outpost supplied remote radar sites watching for a nuclear attack coming over the pole.\\n\\nThese days, military transport planes fly scientists and their equipment across 1,000 kilometers of Arctic ice to East GRIP. They make research possible here and at other far-flung scientific outposts on the vast Greenland ice sheet.\\n\\nDeparting from Kangerlussuaq, VOA visited East GRIP and other remote corners of Greenland with the 109th Airlift Wing of the U.S. Air National Guard for a firsthand look at science in action at the leading edge of climate change.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content=open(\"example.txt\",\"r\")\n",
    "content.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter length of summary: 4\n"
     ]
    }
   ],
   "source": [
    "length = float(input(\"Enter length of summary: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_input(data):\n",
    "    \"\"\" \n",
    "    Currently just a whitespace remover. More thought will have to be given with how \n",
    "    to handle sanitzation and encoding in a way that most text files can be successfully\n",
    "    parsed\n",
    "    \"\"\"\n",
    "    #data = list(data)\n",
    "    #replace = {\n",
    "     #   ord('\\f') : ' ',\n",
    "      #  ord('\\t') : ' ',\n",
    "       # ord('\\n') : ' ',\n",
    "        #ord('\\r') : None\n",
    "    #}\n",
    "\n",
    "    #return data.translate(replace)\n",
    "    content.replace(\" \", \" \")\n",
    "    content.replace(\"\\t\",\" \")\n",
    "    content.replace(\"\\n\",\" \")\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_content(content):\n",
    "    \"\"\"\n",
    "    Accept the content and produce a list of tokenized sentences, \n",
    "    a list of tokenized words, and then a list of the tokenized words\n",
    "    with stop words built from NLTK corpus and Python string class filtred out. \n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    words = word_tokenize(content.lower())\n",
    "    \n",
    "    return [\n",
    "        sent_tokenize(content),\n",
    "        [word for word in words if word not in stop_words]    \n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_tokens(filterd_words, sentence_tokens):\n",
    "    \"\"\"\n",
    "    Builds a frequency map based on the filtered list of words and \n",
    "    uses this to produce a map of each sentence and its total score\n",
    "    \"\"\"\n",
    "    word_freq = FreqDist(filterd_words)\n",
    "\n",
    "    ranking = defaultdict(int)\n",
    "\n",
    "    for i, sentence in enumerate(sentence_tokens):\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_freq:\n",
    "                ranking[i] += word_freq[word]\n",
    "\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(ranks, sentences, length):\n",
    "    \"\"\"\n",
    "    Utilizes a ranking map produced by score_token to extract\n",
    "    the highest ranking sentences in order after converting from\n",
    "    array to string.  \n",
    "    \"\"\"\n",
    "    if int(length) > len(sentences): \n",
    "        print(\"Error, more sentences requested than available. Use --l (--length) flag to adjust.\")\n",
    "        exit()\n",
    "\n",
    "    indexes = nlargest(length, ranks, key=ranks.get)\n",
    "    final_sentences = [sentences[j] for j in sorted(indexes)]\n",
    "    return ' '.join(final_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-15e140838676>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msentence_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msentence_ranks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_ranks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-dec7cd4f4a75>\u001b[0m in \u001b[0;36msanitize_input\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#return data.translate(replace)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "content = sanitize_input(content)\n",
    "sentence_tokens, word_tokens = tokenize_content(content)  \n",
    "sentence_ranks = score_tokens(word_tokens, sentence_tokens)\n",
    "summarize(sentence_ranks, sentence_tokens, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
